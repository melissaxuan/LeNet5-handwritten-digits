{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036718c1",
   "metadata": {},
   "source": [
    "### 1. LeNet5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ScaledTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1.7159 * torch.tanh(x * 2 / 3)\n",
    "    \n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.tanh = ScaledTanh()\n",
    "\n",
    "        # C1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        \n",
    "        # S2\n",
    "        self.weight2 = nn.Parameter(torch.ones(1, 6, 1, 1))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(1, 6, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight2)\n",
    "        nn.init.uniform_(self.weight2, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias2.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C3\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        mask = torch.zeros_like(self.conv3.weight, dtype=torch.bool)\n",
    "        table = self.connection_table()\n",
    "        for out_idx, conn in enumerate(table):\n",
    "            mask[out_idx, conn] = True\n",
    "        self.register_buffer(\"conv3_mask\", mask.float())\n",
    "        with torch.no_grad():\n",
    "            self.conv3.weight *= self.conv3_mask\n",
    "\n",
    "        # S4\n",
    "        self.weight4 = nn.Parameter(torch.ones(1, 16, 1, 1))\n",
    "        self.bias4 = nn.Parameter(torch.zeros(1, 16, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight4)\n",
    "        nn.init.uniform_(self.weight4, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias4.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C5\n",
    "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "\n",
    "        # F6\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "\n",
    "        # Output Layer\n",
    "        self.prototypes = self.compute_rbf_prototypes()\n",
    "\n",
    "    def connection_table(self):\n",
    "        return [\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 3],\n",
    "            [2, 3, 4],\n",
    "            [3, 4, 5],\n",
    "            [0, 4, 5],\n",
    "            [0, 1, 5],\n",
    "            [0, 1, 2, 3],\n",
    "            [1, 2, 3, 4],\n",
    "            [2, 3, 4, 5],\n",
    "            [0, 3, 4, 5],\n",
    "            [0, 1, 4, 5],\n",
    "            [0, 1, 2, 5],\n",
    "            [0, 1, 3, 4],\n",
    "            [1, 2, 4, 5],\n",
    "            [1, 2, 3, 5],\n",
    "            [0, 1, 2, 3, 4, 5]\n",
    "        ]\n",
    "\n",
    "    def compute_rbf_prototypes(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        prototypes = []\n",
    "\n",
    "        image_folder = './digits updated/'\n",
    "        bitmap_size = (7,12)\n",
    "        num_classes = 10\n",
    "\n",
    "        for label in range(num_classes):\n",
    "            class_folder = os.path.join(image_folder, str(label))\n",
    "            images = []\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv2.imread(img_path, 0)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, bitmap_size)\n",
    "                    image = 255.0 - image  # Invert colors\n",
    "                    image = (image > 127).astype(np.float32)  # Binarize to 0 and 1\n",
    "                    image = image / 255.0\n",
    "\n",
    "                    images.append(image)\n",
    "            if images:\n",
    "                mean_image = np.mean(images, axis=0)\n",
    "                prototypes.append(mean_image.flatten())\n",
    "\n",
    "        prototypes_arr = np.array(prototypes)\n",
    "\n",
    "        return torch.tensor(prototypes_arr, dtype=torch.float32)\n",
    "\n",
    "    def compute_rbf_distance(self, x):\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-5)\n",
    "        prototypes = (self.prototypes - self.prototypes.mean(dim=1, keepdim=True)) / (self.prototypes.std(dim=1, keepdim=True) + 1e-5)\n",
    "\n",
    "        # L2 normalize input features and prototypes\n",
    "        x = F.normalize(x, p=2, dim=1)  # shape: [batch_size, feature_dim]\n",
    "        prototypes = F.normalize(prototypes, p=2, dim=1)  # shape: [num_classes, feature_dim]\n",
    "\n",
    "        # Compute pairwise squared Euclidean distances\n",
    "        x_expanded = x.unsqueeze(1)  # shape: [batch_size, 1, feature_dim]\n",
    "        prototypes_expanded = prototypes.unsqueeze(0)  # shape: [1, num_classes, feature_dim]\n",
    "        output = (x_expanded - prototypes_expanded).pow(2).sum(-1)  # shape: [batch_size, num_classes]\n",
    "\n",
    "        return output   \n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # S2\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight2.view(1, -1, 1, 1) + self.bias2.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C3\n",
    "        self.conv3.weight.data *= self.conv3_mask  # Apply the mask to the weights\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # S4\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight4.view(1, -1, 1, 1) + self.bias4.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C5\n",
    "        x = self.conv5(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # F6\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.compute_rbf_distance(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cac26b",
   "metadata": {},
   "source": [
    "### 2. Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "train_image_folder = './data/train/'\n",
    "test_image_folder = './data/test/'\n",
    "train_label_file = './data/train_label.txt'\n",
    "test_label_file = './data/test_label.txt'\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "with open(train_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(train_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            train_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            train_labels.append(label)\n",
    "\n",
    "with open(test_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(test_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            test_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            test_labels.append(label)\n",
    "\n",
    "train_images = torch.stack(train_images)\n",
    "test_images = torch.stack(test_images)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47408a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_images, train_labels), batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_images, test_labels), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca28891",
   "metadata": {},
   "source": [
    "### 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83586ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(outputs, labels, j=0.1):\n",
    "    batch_size = outputs.size(0)\n",
    "\n",
    "    # Correct class distances\n",
    "    pos_dists = outputs[torch.arange(batch_size), labels]  # Shape: [B]\n",
    "\n",
    "    # Mask out correct class\n",
    "    mask = torch.ones_like(outputs, dtype=torch.bool)\n",
    "    mask[torch.arange(batch_size), labels] = False\n",
    "    neg_dists = outputs[mask].view(batch_size, -1)  # Shape: [B, C-1]\n",
    "\n",
    "    # Stable discriminative log-sum-exp term\n",
    "    # log(e^{-j} + sum(e^{-d_i})) = logsumexp([-j, -d1, -d2, ..., -d9])\n",
    "    margin_tensor = torch.full((batch_size, 1), -j, device=outputs.device)\n",
    "    all_terms = torch.cat([margin_tensor, -neg_dists], dim=1)\n",
    "    log_term = torch.logsumexp(all_terms, dim=1)\n",
    "\n",
    "    # Final loss (lower distance is better)\n",
    "    loss = (-pos_dists + log_term).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "691e3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 20.05%, Training Loss: -211172.80%, Testing Accuracy: 60.81%, Testing Loss: -36308.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Accuracy: 73.70%, Training Loss: -219410.00%, Testing Accuracy: 75.60%, Testing Loss: -36723.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Training Accuracy: 80.46%, Training Loss: -220932.91%, Testing Accuracy: 82.03%, Testing Loss: -36932.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Training Accuracy: 85.96%, Training Loss: -222037.30%, Testing Accuracy: 88.37%, Testing Loss: -37082.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Training Accuracy: 90.33%, Training Loss: -222883.52%, Testing Accuracy: 90.79%, Testing Loss: -37178.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Training Accuracy: 92.02%, Training Loss: -223306.96%, Testing Accuracy: 91.82%, Testing Loss: -37234.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Training Accuracy: 92.93%, Training Loss: -223562.76%, Testing Accuracy: 92.63%, Testing Loss: -37272.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Training Accuracy: 93.47%, Training Loss: -223749.74%, Testing Accuracy: 93.17%, Testing Loss: -37301.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Training Accuracy: 93.92%, Training Loss: -223901.74%, Testing Accuracy: 93.56%, Testing Loss: -37324.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Training Accuracy: 94.26%, Training Loss: -224029.75%, Testing Accuracy: 93.94%, Testing Loss: -37343.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Training Accuracy: 94.51%, Training Loss: -224138.90%, Testing Accuracy: 94.29%, Testing Loss: -37359.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Training Accuracy: 94.73%, Training Loss: -224233.80%, Testing Accuracy: 94.41%, Testing Loss: -37374.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Training Accuracy: 94.92%, Training Loss: -224319.25%, Testing Accuracy: 94.64%, Testing Loss: -37387.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Training Accuracy: 95.11%, Training Loss: -224399.52%, Testing Accuracy: 94.83%, Testing Loss: -37400.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Training Accuracy: 95.31%, Training Loss: -224477.56%, Testing Accuracy: 95.01%, Testing Loss: -37414.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Training Accuracy: 95.48%, Training Loss: -224554.21%, Testing Accuracy: 95.30%, Testing Loss: -37427.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Training Accuracy: 95.64%, Training Loss: -224628.36%, Testing Accuracy: 95.56%, Testing Loss: -37440.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Training Accuracy: 95.79%, Training Loss: -224698.48%, Testing Accuracy: 95.78%, Testing Loss: -37452.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Training Accuracy: 95.95%, Training Loss: -224763.84%, Testing Accuracy: 95.96%, Testing Loss: -37464.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Training Accuracy: 96.07%, Training Loss: -224824.32%, Testing Accuracy: 96.07%, Testing Loss: -37474.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define relevant variables\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet5(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "# Initialize lists to store accuracy and error rates for plotting\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = customLoss(outputs, labels)\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tracking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "\n",
    "    # After training the epoch, evaluate on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient tracking during evaluation\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = customLoss(outputs, labels)\n",
    "            test_loss_list.append(loss.item())\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Testing accuracy and error rate for the epoch\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    # Print the results for this epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Training Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Testing Accuracy: {test_accuracy:.2f}%, ')\n",
    "\n",
    "\n",
    "    # Store values for plotting later\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    # print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, total_step, loss.item(), accuracy))\n",
    "\n",
    "    # accuracy = 100 * correct / total\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b58a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedd1b5",
   "metadata": {},
   "source": [
    "### 4. Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee656463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, you can plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), test_accuracies, label='Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing Accuracy')\n",
    "\n",
    "# Plot error rate\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_error_rates, label='Training Error Rate')\n",
    "plt.plot(range(1, num_epochs+1), test_error_rates, label='Testing Error Rate')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error Rate (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing Error Rate')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8cfd4",
   "metadata": {},
   "source": [
    "### 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e249de",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # torch.save(model.state_dict(), 'model_epoch_{}.pth'.format(epoch+1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeNet5-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
