{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036718c1",
   "metadata": {},
   "source": [
    "### 1. LeNet5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "931a1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ScaledTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1.7159 * torch.tanh(x * 2 / 3)\n",
    "    \n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.tanh = ScaledTanh()\n",
    "\n",
    "        # C1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        \n",
    "        # S2\n",
    "        self.weight2 = nn.Parameter(torch.ones(1, 6, 1, 1))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(1, 6, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight2)\n",
    "        nn.init.uniform_(self.weight2, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias2.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C3\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        mask = torch.zeros_like(self.conv3.weight, dtype=torch.bool)\n",
    "        table = self.connection_table()\n",
    "        for out_idx, conn in enumerate(table):\n",
    "            mask[out_idx, conn] = True\n",
    "        self.register_buffer(\"conv3_mask\", mask.float())\n",
    "        with torch.no_grad():\n",
    "            self.conv3.weight *= self.conv3_mask\n",
    "        # self.weight3 = nn.Parameter(torch.Tensor(16, 6, 5, 5))\n",
    "        # self.bias3 = nn.Parameter(torch.Tensor(1, 16, 1, 1))  # Shape [16] instead of [1, 16, 1, 1]\n",
    "        # fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight3)\n",
    "        # nn.init.uniform_(self.weight3, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        # self.bias3.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # S4\n",
    "        self.weight4 = nn.Parameter(torch.ones(1, 16, 1, 1))\n",
    "        self.bias4 = nn.Parameter(torch.zeros(1, 16, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight4)\n",
    "        nn.init.uniform_(self.weight4, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias4.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C5\n",
    "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "\n",
    "        # F6\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "\n",
    "        # Output Layer\n",
    "        self.prototypes = self.compute_rbf_prototypes()\n",
    "        # self.output = nn.Linear(10, 84)\n",
    "\n",
    "    def connection_table(self):\n",
    "        return [\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 3],\n",
    "            [2, 3, 4],\n",
    "            [3, 4, 5],\n",
    "            [0, 4, 5],\n",
    "            [0, 1, 5],\n",
    "            [0, 1, 2, 3],\n",
    "            [1, 2, 3, 4],\n",
    "            [2, 3, 4, 5],\n",
    "            [0, 3, 4, 5],\n",
    "            [0, 1, 4, 5],\n",
    "            [0, 1, 2, 5],\n",
    "            [0, 1, 3, 4],\n",
    "            [1, 2, 4, 5],\n",
    "            [1, 2, 3, 5],\n",
    "            [0, 1, 2, 3, 4, 5]\n",
    "        ]\n",
    "\n",
    "    def compute_rbf_prototypes(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        prototypes = []\n",
    "\n",
    "        image_folder = './digits updated/'\n",
    "        bitmap_size = (7,12)\n",
    "        num_classes = 10\n",
    "\n",
    "        for label in range(num_classes):\n",
    "            class_folder = os.path.join(image_folder, str(label))\n",
    "            images = []\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv2.imread(img_path, 0)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, bitmap_size)\n",
    "                    image = 255.0 - image  # Invert colors\n",
    "                    image = (image > 127).astype(np.float32)  # Binarize to 0 and 1\n",
    "                    image = image / 255.0\n",
    "                    # print(image)\n",
    "                    # plt.imshow(image, cmap='gray')\n",
    "                    # plt.title(\"Image for digit \"+ str(label))\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "                    images.append(image)\n",
    "            if images:\n",
    "                mean_image = np.mean(images, axis=0)\n",
    "                # mean_image = np.mean(images, axis=0) / 255.0  # Normalize grayscale to [0, 1]\n",
    "                prototypes.append(mean_image.flatten())\n",
    "\n",
    "\n",
    "\n",
    "        prototypes_arr = np.array(prototypes)\n",
    "        # for i in range(prototypes_arr.shape[0]):\n",
    "\n",
    "            # proto = prototypes_arr[i].reshape(12, 7)\n",
    "            # plt.imshow(proto, cmap='gray')\n",
    "            # plt.title(\"Prototype for digit \"+ str(i))\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "        # print(\"Prototypes Array Sample (Before Return):\")\n",
    "        # print(prototypes_arr[:5])  # Print the first 5 prototypes for debugging\n",
    "        # print(\"Prototypes Mean/Std:\", prototypes_arr.mean(), prototypes_arr.std())  # Check mean and std\n",
    "\n",
    "        return torch.tensor(prototypes_arr, dtype=torch.float32)\n",
    "\n",
    "    def compute_rbf_distance(self, x):\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-5)\n",
    "        prototypes = (self.prototypes - self.prototypes.mean(dim=1, keepdim=True)) / (self.prototypes.std(dim=1, keepdim=True) + 1e-5)\n",
    "\n",
    "        # L2 normalize input features and prototypes\n",
    "        x = F.normalize(x, p=2, dim=1)  # shape: [batch_size, feature_dim]\n",
    "        prototypes = F.normalize(prototypes, p=2, dim=1)  # shape: [num_classes, feature_dim]\n",
    "\n",
    "        # Compute pairwise squared Euclidean distances\n",
    "        x_expanded = x.unsqueeze(1)  # shape: [batch_size, 1, feature_dim]\n",
    "        prototypes_expanded = prototypes.unsqueeze(0)  # shape: [1, num_classes, feature_dim]\n",
    "        output = (x_expanded - prototypes_expanded).pow(2).sum(-1)  # shape: [batch_size, num_classes]\n",
    "\n",
    "        # Debug info\n",
    "        # print(\"RBF distances (min/max):\", output.min().item(), output.max().item())\n",
    "        # print(\"RBF distances:\", output[0])  # print one example\n",
    "\n",
    "        return output\n",
    "        # x_expanded = x.unsqueeze(1).expand((x.size(0), self.prototypes.size(0), self.prototypes.size(1)))  \n",
    "        # params_expanded = self.prototypes.unsqueeze(0).expand((x.size(0), self.prototypes.size(0), self.prototypes.size(1)))         \n",
    "        # output = (x_expanded - params_expanded).pow(2).sum(-1)\n",
    "\n",
    "        # print(\"RBF distances (min/max):\", output.min().item(), output.max().item())\n",
    "        # print(\"RBF distances:\", output[0])  # print only first example for readability\n",
    "    \n",
    "        # return output    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # S2\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight2.view(1, -1, 1, 1) + self.bias2.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C3\n",
    "        self.conv3.weight.data *= self.conv3_mask  # Apply the mask to the weights\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # print(\"conv3 weights sample:\", self.conv3.weight[0, :3, 2:4, 2:4])\n",
    "        # print(\"conv3 mask sample:\", self.conv3_mask[0, :3, 2:4, 2:4])\n",
    "\n",
    "        # batch_size = x.size(0)\n",
    "        # output = torch.zeros(batch_size, 16, x.size(2) - 5 + 1, x.size(3) - 5 + 1).to(x.device)\n",
    "        # for i in range(16):  # For each output channel\n",
    "        #     connected_channels = self.connection_table()[i]\n",
    "        #     for j, input_channel in enumerate(connected_channels): # input channels 0-5\n",
    "        #         input_slice = x[:, input_channel, :, :].unsqueeze(1)  # Select the input channel and add batch dimension\n",
    "                \n",
    "        #         # Create the weight tensor for the convolution (shape: [1, 1, 5, 5])\n",
    "        #         weight = self.weight3[i, j, :, :].unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "        #         # Perform convolution (output will have shape [batch_size, 1, height, width])\n",
    "        #         conv_output = F.conv2d(input_slice, weight)\n",
    "                \n",
    "        #         # print(conv_output.shape)\n",
    "                \n",
    "        #         # Accumulate results in the correct output channel\n",
    "        #         output[:, i:i+1, :, :] += conv_output\n",
    "        #         # print(output.shape)\n",
    "\n",
    "        # bias = self.bias3.view(16)  # shape: [16]\n",
    "        # for i in range(16):\n",
    "        #     output[:, i:i+1, :, :] += bias[i]\n",
    "        # # print(output.shape)\n",
    "        # x = output\n",
    "        # x = self.tanh(x)\n",
    "\n",
    "        # S4\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight4.view(1, -1, 1, 1) + self.bias4.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C5\n",
    "        x = self.conv5(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # F6\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc6(x)\n",
    "        # print(\"fc6 output (sample):\", x[0][:10])\n",
    "        # print(\"Prototype[0] (sample):\", self.prototypes[0][:10])\n",
    "        # print(\"Feature mean/std:\", x.mean().item(), x.std().item())\n",
    "        # print(\"Prototype mean/std:\", self.prototypes.mean().item(), self.prototypes.std().item())\n",
    "        # Output Layer\n",
    "        x = self.compute_rbf_distance(x)\n",
    "        # x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cac26b",
   "metadata": {},
   "source": [
    "### 2. Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "820d833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "train_image_folder = './data/train/'\n",
    "test_image_folder = './data/test/'\n",
    "train_label_file = './data/train_label.txt'\n",
    "test_label_file = './data/test_label.txt'\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "with open(train_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(train_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            train_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            train_labels.append(label)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "with open(test_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(test_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            test_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            test_labels.append(label)\n",
    "            # import matplotlib.pyplot as plt\n",
    "\n",
    "            # # Assuming 'image' is your input image\n",
    "            # plt.imshow(image.squeeze(0), cmap='gray')  # Display image in grayscale\n",
    "            # plt.colorbar()  # Add a color bar to show the range of pixel values\n",
    "            # plt.title(\"Image Preview for \" + str(label))  # Optional title\n",
    "            # plt.show()\n",
    "\n",
    "train_images = torch.stack(train_images)\n",
    "test_images = torch.stack(test_images)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "47408a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_images, train_labels), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_images, test_labels), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca28891",
   "metadata": {},
   "source": [
    "### 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "83586ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(outputs, labels):\n",
    "    # print(\"outputs min/max:\", outputs.min().item(), outputs.max().item())\n",
    "    # print(\"outputs:\", outputs)\n",
    "    # print(\"Labels:\", labels)\n",
    "    assert labels.min() >= 0 and labels.max() < 10\n",
    "\n",
    "    outputs = torch.clamp(outputs, min=-20, max=20)\n",
    "    outputsC = outputs[torch.arange(outputs.size(0)), labels]  # shape [batch]\n",
    "\n",
    "    # outputsC = outputs[:, labels]\n",
    "    mask = torch.ones_like(outputs, dtype=torch.bool)\n",
    "    mask[:, labels] = False\n",
    "    wrong = outputs[mask].view(outputs.size(0), -1)\n",
    "    log_sum = torch.logsumexp(-(wrong - 0.1), dim=1)\n",
    "    \n",
    "    return (outputsC + log_sum).mean()\n",
    "    # predicted_class = torch.argmax(outputs, dim=1)  # Get the predicted class (32,)\n",
    "    # # correct_predictions = (predicted_class == labels)  # Tensor of booleans (32,)\n",
    "    # # print(\"predicted class:\", predicted_class, \"labels:\", labels)\n",
    "    # loss = outputs[labels==predicted_class] # .pow(2).sum() # correct classes\n",
    "    # # print(\"loss1:\", loss)\n",
    "    # loss += torch.log(np.exp(-0.1) + torch.exp(-outputs[labels!=predicted_class].sum())) # incorrect classes\n",
    "    # # print(\"loss2:\", loss)\n",
    "    # loss /= 10 # normalize by number of classes\n",
    "    # # print(\"loss3:\", loss)\n",
    "    # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "691e3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 1875\n",
      "Epoch [1/10], Step [200/1875], Loss: 2.3060, Training Accuracy: 10.06%\n",
      "Epoch [1/10], Step [400/1875], Loss: 2.2855, Training Accuracy: 10.25%\n",
      "Epoch [1/10], Step [600/1875], Loss: 2.3131, Training Accuracy: 10.65%\n",
      "Epoch [1/10], Step [800/1875], Loss: 2.2696, Training Accuracy: 10.88%\n",
      "Epoch [1/10], Step [1000/1875], Loss: 1.6228, Training Accuracy: 22.03%\n",
      "Epoch [1/10], Step [1200/1875], Loss: 1.6222, Training Accuracy: 31.52%\n",
      "Epoch [1/10], Step [1400/1875], Loss: 1.4386, Training Accuracy: 38.67%\n",
      "Epoch [1/10], Step [1600/1875], Loss: 1.4863, Training Accuracy: 44.25%\n",
      "Epoch [1/10], Step [1800/1875], Loss: 1.4434, Training Accuracy: 49.07%\n",
      "Epoch [1/10], Training Accuracy: 50.81%\n",
      "Epoch [2/10], Step [200/1875], Loss: 1.4231, Training Accuracy: 89.98%\n",
      "Epoch [2/10], Step [400/1875], Loss: 1.4547, Training Accuracy: 89.84%\n",
      "Epoch [2/10], Step [600/1875], Loss: 1.4626, Training Accuracy: 90.16%\n",
      "Epoch [2/10], Step [800/1875], Loss: 1.4802, Training Accuracy: 90.54%\n",
      "Epoch [2/10], Step [1000/1875], Loss: 1.4573, Training Accuracy: 90.68%\n",
      "Epoch [2/10], Step [1200/1875], Loss: 1.5311, Training Accuracy: 91.01%\n",
      "Epoch [2/10], Step [1400/1875], Loss: 1.3718, Training Accuracy: 91.28%\n",
      "Epoch [2/10], Step [1600/1875], Loss: 1.3958, Training Accuracy: 91.38%\n",
      "Epoch [2/10], Step [1800/1875], Loss: 1.3788, Training Accuracy: 91.66%\n",
      "Epoch [2/10], Training Accuracy: 91.82%\n",
      "Epoch [3/10], Step [200/1875], Loss: 1.3868, Training Accuracy: 93.92%\n",
      "Epoch [3/10], Step [400/1875], Loss: 1.4005, Training Accuracy: 93.56%\n",
      "Epoch [3/10], Step [600/1875], Loss: 1.4227, Training Accuracy: 93.52%\n",
      "Epoch [3/10], Step [800/1875], Loss: 1.4454, Training Accuracy: 93.66%\n",
      "Epoch [3/10], Step [1000/1875], Loss: 1.4202, Training Accuracy: 93.56%\n",
      "Epoch [3/10], Step [1200/1875], Loss: 1.5238, Training Accuracy: 93.72%\n",
      "Epoch [3/10], Step [1400/1875], Loss: 1.3550, Training Accuracy: 93.83%\n",
      "Epoch [3/10], Step [1600/1875], Loss: 1.3675, Training Accuracy: 93.84%\n",
      "Epoch [3/10], Step [1800/1875], Loss: 1.3616, Training Accuracy: 93.99%\n",
      "Epoch [3/10], Training Accuracy: 94.09%\n",
      "Epoch [4/10], Step [200/1875], Loss: 1.3681, Training Accuracy: 94.95%\n",
      "Epoch [4/10], Step [400/1875], Loss: 1.3811, Training Accuracy: 94.80%\n",
      "Epoch [4/10], Step [600/1875], Loss: 1.4112, Training Accuracy: 94.73%\n",
      "Epoch [4/10], Step [800/1875], Loss: 1.4323, Training Accuracy: 94.82%\n",
      "Epoch [4/10], Step [1000/1875], Loss: 1.3906, Training Accuracy: 94.75%\n",
      "Epoch [4/10], Step [1200/1875], Loss: 1.5034, Training Accuracy: 94.89%\n",
      "Epoch [4/10], Step [1400/1875], Loss: 1.3468, Training Accuracy: 94.96%\n",
      "Epoch [4/10], Step [1600/1875], Loss: 1.3491, Training Accuracy: 94.95%\n",
      "Epoch [4/10], Step [1800/1875], Loss: 1.3448, Training Accuracy: 95.08%\n",
      "Epoch [4/10], Training Accuracy: 95.15%\n",
      "Epoch [5/10], Step [200/1875], Loss: 1.3541, Training Accuracy: 96.00%\n",
      "Epoch [5/10], Step [400/1875], Loss: 1.3589, Training Accuracy: 95.80%\n",
      "Epoch [5/10], Step [600/1875], Loss: 1.4019, Training Accuracy: 95.79%\n",
      "Epoch [5/10], Step [800/1875], Loss: 1.4129, Training Accuracy: 95.85%\n",
      "Epoch [5/10], Step [1000/1875], Loss: 1.3614, Training Accuracy: 95.78%\n",
      "Epoch [5/10], Step [1200/1875], Loss: 1.4808, Training Accuracy: 95.88%\n",
      "Epoch [5/10], Step [1400/1875], Loss: 1.3341, Training Accuracy: 95.92%\n",
      "Epoch [5/10], Step [1600/1875], Loss: 1.3322, Training Accuracy: 95.88%\n",
      "Epoch [5/10], Step [1800/1875], Loss: 1.3225, Training Accuracy: 95.98%\n",
      "Epoch [5/10], Training Accuracy: 96.04%\n",
      "Epoch [6/10], Step [200/1875], Loss: 1.3466, Training Accuracy: 96.66%\n",
      "Epoch [6/10], Step [400/1875], Loss: 1.3385, Training Accuracy: 96.43%\n",
      "Epoch [6/10], Step [600/1875], Loss: 1.3866, Training Accuracy: 96.47%\n",
      "Epoch [6/10], Step [800/1875], Loss: 1.3930, Training Accuracy: 96.48%\n",
      "Epoch [6/10], Step [1000/1875], Loss: 1.3395, Training Accuracy: 96.42%\n",
      "Epoch [6/10], Step [1200/1875], Loss: 1.4659, Training Accuracy: 96.51%\n",
      "Epoch [6/10], Step [1400/1875], Loss: 1.3240, Training Accuracy: 96.54%\n",
      "Epoch [6/10], Step [1600/1875], Loss: 1.3210, Training Accuracy: 96.48%\n",
      "Epoch [6/10], Step [1800/1875], Loss: 1.3085, Training Accuracy: 96.56%\n",
      "Epoch [6/10], Training Accuracy: 96.61%\n",
      "Epoch [7/10], Step [200/1875], Loss: 1.3409, Training Accuracy: 97.25%\n",
      "Epoch [7/10], Step [400/1875], Loss: 1.3222, Training Accuracy: 97.02%\n",
      "Epoch [7/10], Step [600/1875], Loss: 1.3685, Training Accuracy: 97.05%\n",
      "Epoch [7/10], Step [800/1875], Loss: 1.3791, Training Accuracy: 97.05%\n",
      "Epoch [7/10], Step [1000/1875], Loss: 1.3260, Training Accuracy: 96.97%\n",
      "Epoch [7/10], Step [1200/1875], Loss: 1.4539, Training Accuracy: 97.03%\n",
      "Epoch [7/10], Step [1400/1875], Loss: 1.3172, Training Accuracy: 97.05%\n",
      "Epoch [7/10], Step [1600/1875], Loss: 1.3112, Training Accuracy: 97.00%\n",
      "Epoch [7/10], Step [1800/1875], Loss: 1.3015, Training Accuracy: 97.06%\n",
      "Epoch [7/10], Training Accuracy: 97.10%\n",
      "Epoch [8/10], Step [200/1875], Loss: 1.3367, Training Accuracy: 97.64%\n",
      "Epoch [8/10], Step [400/1875], Loss: 1.3101, Training Accuracy: 97.41%\n",
      "Epoch [8/10], Step [600/1875], Loss: 1.3537, Training Accuracy: 97.39%\n",
      "Epoch [8/10], Step [800/1875], Loss: 1.3701, Training Accuracy: 97.41%\n",
      "Epoch [8/10], Step [1000/1875], Loss: 1.3180, Training Accuracy: 97.35%\n",
      "Epoch [8/10], Step [1200/1875], Loss: 1.4442, Training Accuracy: 97.38%\n",
      "Epoch [8/10], Step [1400/1875], Loss: 1.3128, Training Accuracy: 97.40%\n",
      "Epoch [8/10], Step [1600/1875], Loss: 1.3029, Training Accuracy: 97.35%\n",
      "Epoch [8/10], Step [1800/1875], Loss: 1.2977, Training Accuracy: 97.41%\n",
      "Epoch [8/10], Training Accuracy: 97.45%\n",
      "Epoch [9/10], Step [200/1875], Loss: 1.3340, Training Accuracy: 97.86%\n",
      "Epoch [9/10], Step [400/1875], Loss: 1.3017, Training Accuracy: 97.62%\n",
      "Epoch [9/10], Step [600/1875], Loss: 1.3430, Training Accuracy: 97.63%\n",
      "Epoch [9/10], Step [800/1875], Loss: 1.3641, Training Accuracy: 97.63%\n",
      "Epoch [9/10], Step [1000/1875], Loss: 1.3129, Training Accuracy: 97.59%\n",
      "Epoch [9/10], Step [1200/1875], Loss: 1.4364, Training Accuracy: 97.64%\n",
      "Epoch [9/10], Step [1400/1875], Loss: 1.3097, Training Accuracy: 97.64%\n",
      "Epoch [9/10], Step [1600/1875], Loss: 1.2965, Training Accuracy: 97.59%\n",
      "Epoch [9/10], Step [1800/1875], Loss: 1.2953, Training Accuracy: 97.63%\n",
      "Epoch [9/10], Training Accuracy: 97.67%\n",
      "Epoch [10/10], Step [200/1875], Loss: 1.3323, Training Accuracy: 97.97%\n",
      "Epoch [10/10], Step [400/1875], Loss: 1.2959, Training Accuracy: 97.76%\n",
      "Epoch [10/10], Step [600/1875], Loss: 1.3361, Training Accuracy: 97.82%\n",
      "Epoch [10/10], Step [800/1875], Loss: 1.3600, Training Accuracy: 97.84%\n",
      "Epoch [10/10], Step [1000/1875], Loss: 1.3090, Training Accuracy: 97.80%\n",
      "Epoch [10/10], Step [1200/1875], Loss: 1.4300, Training Accuracy: 97.84%\n",
      "Epoch [10/10], Step [1400/1875], Loss: 1.3074, Training Accuracy: 97.84%\n",
      "Epoch [10/10], Step [1600/1875], Loss: 1.2916, Training Accuracy: 97.79%\n",
      "Epoch [10/10], Step [1800/1875], Loss: 1.2937, Training Accuracy: 97.84%\n",
      "Epoch [10/10], Training Accuracy: 97.87%\n"
     ]
    }
   ],
   "source": [
    "# Define relevant variables\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet5(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "print(f'Total steps: {total_step}')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(\"Input image min/max:\", images.min(), images.max())\n",
    "        # import matplotlib.pyplot as plt\n",
    "\n",
    "        # Assuming 'image' is your input image\n",
    "        # plt.imshow(images[0].squeeze(0), cmap='gray')  # Display image in grayscale\n",
    "        # plt.colorbar()  # Add a color bar to show the range of pixel values\n",
    "        # plt.title(\"Image Preview\")  # Optional title\n",
    "        # plt.show()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(labels.shape, outputs.shape)\n",
    "        # print(\"Labels (sample):\", labels[0])\n",
    "        # max_value, predicted_class = torch.max(outputs.data, 1)\n",
    "\n",
    "        # print(f\"Predicted class: {predicted_class[0]}, with value: {max_value[0].item()}\")\n",
    "\n",
    "        # print(\"Raw outputs (distances) sample:\", outputs[0])  # One example\n",
    "        # print(\"Output shape:\", outputs.shape)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # loss = customLoss(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tracking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            accuracy = 100 * correct / total\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, total_step, loss.item(), accuracy))\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b58a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a79fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeNet5-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
