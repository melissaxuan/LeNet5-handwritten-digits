{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036718c1",
   "metadata": {},
   "source": [
    "### 1. LeNet5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "931a1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ScaledTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1.7159 * torch.tanh(x * 2 / 3)\n",
    "    \n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.tanh = ScaledTanh()\n",
    "\n",
    "        # C1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        \n",
    "        # S2\n",
    "        self.weight2 = nn.Parameter(torch.ones(1, 6, 1, 1))\n",
    "        self.bias2 = nn.Parameter(torch.zeros(1, 6, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight2)\n",
    "        nn.init.uniform_(self.weight2, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias2.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C3\n",
    "        self.conv3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        mask = torch.zeros_like(self.conv3.weight, dtype=torch.bool)\n",
    "        table = self.connection_table()\n",
    "        for out_idx, conn in enumerate(table):\n",
    "            mask[out_idx, conn] = True\n",
    "        self.register_buffer(\"conv3_mask\", mask.float())\n",
    "        with torch.no_grad():\n",
    "            self.conv3.weight *= self.conv3_mask\n",
    "        # self.weight3 = nn.Parameter(torch.Tensor(16, 6, 5, 5))\n",
    "        # self.bias3 = nn.Parameter(torch.Tensor(1, 16, 1, 1))  # Shape [16] instead of [1, 16, 1, 1]\n",
    "        # fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight3)\n",
    "        # nn.init.uniform_(self.weight3, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        # self.bias3.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # S4\n",
    "        self.weight4 = nn.Parameter(torch.ones(1, 16, 1, 1))\n",
    "        self.bias4 = nn.Parameter(torch.zeros(1, 16, 1, 1))\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight4)\n",
    "        nn.init.uniform_(self.weight4, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias4.data.fill_(2.4 / fan_in)\n",
    "\n",
    "        # C5\n",
    "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "\n",
    "        # F6\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "\n",
    "        # Output Layer\n",
    "        self.prototypes = self.compute_rbf_prototypes()\n",
    "        # self.output = nn.Linear(10, 84)\n",
    "\n",
    "    def connection_table(self):\n",
    "        return [\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 3],\n",
    "            [2, 3, 4],\n",
    "            [3, 4, 5],\n",
    "            [0, 4, 5],\n",
    "            [0, 1, 5],\n",
    "            [0, 1, 2, 3],\n",
    "            [1, 2, 3, 4],\n",
    "            [2, 3, 4, 5],\n",
    "            [0, 3, 4, 5],\n",
    "            [0, 1, 4, 5],\n",
    "            [0, 1, 2, 5],\n",
    "            [0, 1, 3, 4],\n",
    "            [1, 2, 4, 5],\n",
    "            [1, 2, 3, 5],\n",
    "            [0, 1, 2, 3, 4, 5]\n",
    "        ]\n",
    "\n",
    "    def compute_rbf_prototypes(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        prototypes = []\n",
    "\n",
    "        image_folder = './digits updated/'\n",
    "        bitmap_size = (7,12)\n",
    "        num_classes = 10\n",
    "\n",
    "        for label in range(num_classes):\n",
    "            class_folder = os.path.join(image_folder, str(label))\n",
    "            images = []\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv2.imread(img_path, 0)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, bitmap_size)\n",
    "                    image = 255.0 - image  # Invert colors\n",
    "                    image = (image > 127).astype(np.float32)  # Binarize to 0 and 1\n",
    "                    image = image / 255.0\n",
    "                    # print(image)\n",
    "                    # plt.imshow(image, cmap='gray')\n",
    "                    # plt.title(\"Image for digit \"+ str(label))\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "                    images.append(image)\n",
    "            if images:\n",
    "                mean_image = np.mean(images, axis=0)\n",
    "                # mean_image = np.mean(images, axis=0) / 255.0  # Normalize grayscale to [0, 1]\n",
    "                prototypes.append(mean_image.flatten())\n",
    "\n",
    "\n",
    "\n",
    "        prototypes_arr = np.array(prototypes)\n",
    "        # for i in range(prototypes_arr.shape[0]):\n",
    "\n",
    "            # proto = prototypes_arr[i].reshape(12, 7)\n",
    "            # plt.imshow(proto, cmap='gray')\n",
    "            # plt.title(\"Prototype for digit \"+ str(i))\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "        # print(\"Prototypes Array Sample (Before Return):\")\n",
    "        # print(prototypes_arr[:5])  # Print the first 5 prototypes for debugging\n",
    "        # print(\"Prototypes Mean/Std:\", prototypes_arr.mean(), prototypes_arr.std())  # Check mean and std\n",
    "\n",
    "        return torch.tensor(prototypes_arr, dtype=torch.float32)\n",
    "\n",
    "    def compute_rbf_distance(self, x):\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-5)\n",
    "        prototypes = (self.prototypes - self.prototypes.mean(dim=1, keepdim=True)) / (self.prototypes.std(dim=1, keepdim=True) + 1e-5)\n",
    "\n",
    "        # L2 normalize input features and prototypes\n",
    "        x = F.normalize(x, p=2, dim=1)  # shape: [batch_size, feature_dim]\n",
    "        prototypes = F.normalize(prototypes, p=2, dim=1)  # shape: [num_classes, feature_dim]\n",
    "\n",
    "        # Compute pairwise squared Euclidean distances\n",
    "        x_expanded = x.unsqueeze(1)  # shape: [batch_size, 1, feature_dim]\n",
    "        prototypes_expanded = prototypes.unsqueeze(0)  # shape: [1, num_classes, feature_dim]\n",
    "        output = (x_expanded - prototypes_expanded).pow(2).sum(-1)  # shape: [batch_size, num_classes]\n",
    "\n",
    "        # Debug info\n",
    "        # print(\"RBF distances (min/max):\", output.min().item(), output.max().item())\n",
    "        # print(\"RBF distances:\", output[0])  # print one example\n",
    "\n",
    "        return output\n",
    "        # x_expanded = x.unsqueeze(1).expand((x.size(0), self.prototypes.size(0), self.prototypes.size(1)))  \n",
    "        # params_expanded = self.prototypes.unsqueeze(0).expand((x.size(0), self.prototypes.size(0), self.prototypes.size(1)))         \n",
    "        # output = (x_expanded - params_expanded).pow(2).sum(-1)\n",
    "\n",
    "        # print(\"RBF distances (min/max):\", output.min().item(), output.max().item())\n",
    "        # print(\"RBF distances:\", output[0])  # print only first example for readability\n",
    "    \n",
    "        # return output    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = self.conv1(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # S2\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight2.view(1, -1, 1, 1) + self.bias2.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C3\n",
    "        self.conv3.weight.data *= self.conv3_mask  # Apply the mask to the weights\n",
    "        x = self.conv3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # print(\"conv3 weights sample:\", self.conv3.weight[0, :3, 2:4, 2:4])\n",
    "        # print(\"conv3 mask sample:\", self.conv3_mask[0, :3, 2:4, 2:4])\n",
    "\n",
    "        # batch_size = x.size(0)\n",
    "        # output = torch.zeros(batch_size, 16, x.size(2) - 5 + 1, x.size(3) - 5 + 1).to(x.device)\n",
    "        # for i in range(16):  # For each output channel\n",
    "        #     connected_channels = self.connection_table()[i]\n",
    "        #     for j, input_channel in enumerate(connected_channels): # input channels 0-5\n",
    "        #         input_slice = x[:, input_channel, :, :].unsqueeze(1)  # Select the input channel and add batch dimension\n",
    "                \n",
    "        #         # Create the weight tensor for the convolution (shape: [1, 1, 5, 5])\n",
    "        #         weight = self.weight3[i, j, :, :].unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "        #         # Perform convolution (output will have shape [batch_size, 1, height, width])\n",
    "        #         conv_output = F.conv2d(input_slice, weight)\n",
    "                \n",
    "        #         # print(conv_output.shape)\n",
    "                \n",
    "        #         # Accumulate results in the correct output channel\n",
    "        #         output[:, i:i+1, :, :] += conv_output\n",
    "        #         # print(output.shape)\n",
    "\n",
    "        # bias = self.bias3.view(16)  # shape: [16]\n",
    "        # for i in range(16):\n",
    "        #     output[:, i:i+1, :, :] += bias[i]\n",
    "        # # print(output.shape)\n",
    "        # x = output\n",
    "        # x = self.tanh(x)\n",
    "\n",
    "        # S4\n",
    "        x = F.avg_pool2d(x, kernel_size=2, stride=2) * self.weight4.view(1, -1, 1, 1) + self.bias4.view(1, -1, 1, 1)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # C5\n",
    "        x = self.conv5(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        # F6\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc6(x)\n",
    "        # print(\"fc6 output (sample):\", x[0][:10])\n",
    "        # print(\"Prototype[0] (sample):\", self.prototypes[0][:10])\n",
    "        # print(\"Feature mean/std:\", x.mean().item(), x.std().item())\n",
    "        # print(\"Prototype mean/std:\", self.prototypes.mean().item(), self.prototypes.std().item())\n",
    "        # Output Layer\n",
    "        x = self.compute_rbf_distance(x)\n",
    "        # x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cac26b",
   "metadata": {},
   "source": [
    "### 2. Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "820d833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "train_image_folder = './data/train/'\n",
    "test_image_folder = './data/test/'\n",
    "train_label_file = './data/train_label.txt'\n",
    "test_label_file = './data/test_label.txt'\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "with open(train_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(train_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            train_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            train_labels.append(label)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "with open(test_label_file, 'r') as f:\n",
    "    label_lines = f.readlines()\n",
    "    image_filenames = sorted(os.listdir(train_image_folder))\n",
    "\n",
    "    for idx in range(len(label_lines)):\n",
    "        img_name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(test_image_folder, img_name)\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is not None:\n",
    "            image = Image.fromarray(img)\n",
    "            image = transform(image)\n",
    "            test_images.append(image)\n",
    "            label = int(label_lines[idx].strip())\n",
    "            test_labels.append(label)\n",
    "            # import matplotlib.pyplot as plt\n",
    "\n",
    "            # # Assuming 'image' is your input image\n",
    "            # plt.imshow(image.squeeze(0), cmap='gray')  # Display image in grayscale\n",
    "            # plt.colorbar()  # Add a color bar to show the range of pixel values\n",
    "            # plt.title(\"Image Preview for \" + str(label))  # Optional title\n",
    "            # plt.show()\n",
    "\n",
    "train_images = torch.stack(train_images)\n",
    "test_images = torch.stack(test_images)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "47408a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(train_images, train_labels), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_images, test_labels), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca28891",
   "metadata": {},
   "source": [
    "### 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "83586ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(outputs, labels, j=0.1):\n",
    "    # outputs: distances (lower = better)\n",
    "    batch_size = outputs.size(0)\n",
    "\n",
    "    # Correct class distances\n",
    "    pos_dists = outputs[torch.arange(batch_size), labels]  # Shape: [B]\n",
    "\n",
    "    # Mask out correct class\n",
    "    mask = torch.ones_like(outputs, dtype=torch.bool)\n",
    "    mask[torch.arange(batch_size), labels] = False\n",
    "    neg_dists = outputs[mask].view(batch_size, -1)  # Shape: [B, C-1]\n",
    "\n",
    "    # Stable discriminative log-sum-exp term\n",
    "    # log(e^{-j} + sum(e^{-d_i})) = logsumexp([-j, -d1, -d2, ..., -d9])\n",
    "    margin_tensor = torch.full((batch_size, 1), -j, device=outputs.device)\n",
    "    all_terms = torch.cat([margin_tensor, -neg_dists], dim=1)\n",
    "    log_term = torch.logsumexp(all_terms, dim=1)\n",
    "\n",
    "    # Final loss\n",
    "    loss = (-pos_dists + log_term).mean()\n",
    "    return loss\n",
    "\n",
    "    # pred = outputs[torch.arange(outputs.size(0)), labels]  # shape [batch]\n",
    "    # mask = torch.ones_like(outputs, dtype=torch.bool)\n",
    "    # mask[:, labels] = False\n",
    "    # wrong = outputs[mask].view(outputs.size(0), -1)\n",
    "    # log_sum = torch.logsumexp(-(wrong - 0.1), dim=1)\n",
    "    # return (pred + log_sum).mean()\n",
    "\n",
    "    # print(\"outputs min/max:\", outputs.min().item(), outputs.max().item())\n",
    "    # print(\"outputs:\", outputs)\n",
    "    # print(\"Labels:\", labels)\n",
    "    # assert labels.min() >= 0 and labels.max() < 10\n",
    "\n",
    "    # outputs = torch.clamp(outputs, min=-20, max=20)\n",
    "    # outputsC = outputs[torch.arange(outputs.size(0)), labels]  # shape [batch]\n",
    "\n",
    "    # # outputsC = outputs[:, labels]\n",
    "    # mask = torch.ones_like(outputs, dtype=torch.bool)\n",
    "    # mask[:, labels] = False\n",
    "    # wrong = outputs[mask].view(outputs.size(0), -1)\n",
    "    # log_sum = torch.logsumexp(-(wrong - 0.1), dim=1)\n",
    "    \n",
    "    # # return (outputsC + log_sum).mean()\n",
    "    # predicted_class = torch.argmax(outputs, dim=1)  # Get the predicted class (32,)\n",
    "    # # correct_predictions = (predicted_class == labels)  # Tensor of booleans (32,)\n",
    "    # # print(\"predicted class:\", predicted_class, \"labels:\", labels)\n",
    "    # loss = outputs[labels==predicted_class] # .pow(2).sum() # correct classes\n",
    "    # # print(\"loss1:\", loss)\n",
    "    # loss += torch.log(np.exp(-0.1) + torch.exp(-outputs[labels!=predicted_class].sum())) # incorrect classes\n",
    "    # # print(\"loss2:\", loss)\n",
    "    # loss /= 10 # normalize by number of classes\n",
    "    # # print(\"loss3:\", loss)\n",
    "    # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "691e3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 1875\n",
      "Epoch [1/10], Step [200/1875], Loss: -3.4814, Training Accuracy: 9.17%\n",
      "Epoch [1/10], Step [400/1875], Loss: -3.4652, Training Accuracy: 9.43%\n",
      "Epoch [1/10], Step [600/1875], Loss: -3.4824, Training Accuracy: 9.62%\n",
      "Epoch [1/10], Step [800/1875], Loss: -3.5054, Training Accuracy: 9.59%\n",
      "Epoch [1/10], Step [1000/1875], Loss: -3.4729, Training Accuracy: 9.59%\n",
      "Epoch [1/10], Step [1200/1875], Loss: -3.5045, Training Accuracy: 9.65%\n",
      "Epoch [1/10], Step [1400/1875], Loss: -3.4791, Training Accuracy: 9.66%\n",
      "Epoch [1/10], Step [1600/1875], Loss: -3.4495, Training Accuracy: 9.71%\n",
      "Epoch [1/10], Step [1800/1875], Loss: -3.4747, Training Accuracy: 9.76%\n",
      "Epoch [1/10], Training Accuracy: 9.75%\n",
      "Epoch [2/10], Step [200/1875], Loss: -3.6342, Training Accuracy: 49.52%\n",
      "Epoch [2/10], Step [400/1875], Loss: -3.6408, Training Accuracy: 60.16%\n",
      "Epoch [2/10], Step [600/1875], Loss: -3.6398, Training Accuracy: 64.44%\n",
      "Epoch [2/10], Step [800/1875], Loss: -3.6770, Training Accuracy: 67.92%\n",
      "Epoch [2/10], Step [1000/1875], Loss: -3.6800, Training Accuracy: 70.14%\n",
      "Epoch [2/10], Step [1200/1875], Loss: -3.6766, Training Accuracy: 72.12%\n",
      "Epoch [2/10], Step [1400/1875], Loss: -3.7190, Training Accuracy: 73.77%\n",
      "Epoch [2/10], Step [1600/1875], Loss: -3.6765, Training Accuracy: 75.04%\n",
      "Epoch [2/10], Step [1800/1875], Loss: -3.7006, Training Accuracy: 76.40%\n",
      "Epoch [2/10], Training Accuracy: 77.00%\n",
      "Epoch [3/10], Step [200/1875], Loss: -3.7373, Training Accuracy: 87.98%\n",
      "Epoch [3/10], Step [400/1875], Loss: -3.7018, Training Accuracy: 87.65%\n",
      "Epoch [3/10], Step [600/1875], Loss: -3.6994, Training Accuracy: 87.60%\n",
      "Epoch [3/10], Step [800/1875], Loss: -3.7201, Training Accuracy: 88.05%\n",
      "Epoch [3/10], Step [1000/1875], Loss: -3.7150, Training Accuracy: 88.19%\n",
      "Epoch [3/10], Step [1200/1875], Loss: -3.7029, Training Accuracy: 88.51%\n",
      "Epoch [3/10], Step [1400/1875], Loss: -3.7433, Training Accuracy: 88.82%\n",
      "Epoch [3/10], Step [1600/1875], Loss: -3.7033, Training Accuracy: 88.88%\n",
      "Epoch [3/10], Step [1800/1875], Loss: -3.7214, Training Accuracy: 89.23%\n",
      "Epoch [3/10], Training Accuracy: 89.41%\n",
      "Epoch [4/10], Step [200/1875], Loss: -3.7452, Training Accuracy: 92.55%\n",
      "Epoch [4/10], Step [400/1875], Loss: -3.7164, Training Accuracy: 91.96%\n",
      "Epoch [4/10], Step [600/1875], Loss: -3.7122, Training Accuracy: 92.00%\n",
      "Epoch [4/10], Step [800/1875], Loss: -3.7342, Training Accuracy: 92.23%\n",
      "Epoch [4/10], Step [1000/1875], Loss: -3.7270, Training Accuracy: 92.24%\n",
      "Epoch [4/10], Step [1200/1875], Loss: -3.7141, Training Accuracy: 92.46%\n",
      "Epoch [4/10], Step [1400/1875], Loss: -3.7514, Training Accuracy: 92.63%\n",
      "Epoch [4/10], Step [1600/1875], Loss: -3.7179, Training Accuracy: 92.62%\n",
      "Epoch [4/10], Step [1800/1875], Loss: -3.7331, Training Accuracy: 92.82%\n",
      "Epoch [4/10], Training Accuracy: 92.93%\n",
      "Epoch [5/10], Step [200/1875], Loss: -3.7504, Training Accuracy: 94.86%\n",
      "Epoch [5/10], Step [400/1875], Loss: -3.7248, Training Accuracy: 94.41%\n",
      "Epoch [5/10], Step [600/1875], Loss: -3.7218, Training Accuracy: 94.38%\n",
      "Epoch [5/10], Step [800/1875], Loss: -3.7428, Training Accuracy: 94.46%\n",
      "Epoch [5/10], Step [1000/1875], Loss: -3.7343, Training Accuracy: 94.35%\n",
      "Epoch [5/10], Step [1200/1875], Loss: -3.7202, Training Accuracy: 94.48%\n",
      "Epoch [5/10], Step [1400/1875], Loss: -3.7550, Training Accuracy: 94.54%\n",
      "Epoch [5/10], Step [1600/1875], Loss: -3.7250, Training Accuracy: 94.47%\n",
      "Epoch [5/10], Step [1800/1875], Loss: -3.7379, Training Accuracy: 94.55%\n",
      "Epoch [5/10], Training Accuracy: 94.64%\n",
      "Epoch [6/10], Step [200/1875], Loss: -3.7544, Training Accuracy: 95.58%\n",
      "Epoch [6/10], Step [400/1875], Loss: -3.7321, Training Accuracy: 95.21%\n",
      "Epoch [6/10], Step [600/1875], Loss: -3.7282, Training Accuracy: 95.16%\n",
      "Epoch [6/10], Step [800/1875], Loss: -3.7484, Training Accuracy: 95.20%\n",
      "Epoch [6/10], Step [1000/1875], Loss: -3.7407, Training Accuracy: 95.09%\n",
      "Epoch [6/10], Step [1200/1875], Loss: -3.7264, Training Accuracy: 95.21%\n",
      "Epoch [6/10], Step [1400/1875], Loss: -3.7571, Training Accuracy: 95.26%\n",
      "Epoch [6/10], Step [1600/1875], Loss: -3.7299, Training Accuracy: 95.19%\n",
      "Epoch [6/10], Step [1800/1875], Loss: -3.7421, Training Accuracy: 95.26%\n",
      "Epoch [6/10], Training Accuracy: 95.32%\n",
      "Epoch [7/10], Step [200/1875], Loss: -3.7578, Training Accuracy: 96.03%\n",
      "Epoch [7/10], Step [400/1875], Loss: -3.7386, Training Accuracy: 95.74%\n",
      "Epoch [7/10], Step [600/1875], Loss: -3.7339, Training Accuracy: 95.64%\n",
      "Epoch [7/10], Step [800/1875], Loss: -3.7528, Training Accuracy: 95.70%\n",
      "Epoch [7/10], Step [1000/1875], Loss: -3.7472, Training Accuracy: 95.63%\n",
      "Epoch [7/10], Step [1200/1875], Loss: -3.7318, Training Accuracy: 95.75%\n",
      "Epoch [7/10], Step [1400/1875], Loss: -3.7591, Training Accuracy: 95.80%\n",
      "Epoch [7/10], Step [1600/1875], Loss: -3.7350, Training Accuracy: 95.74%\n",
      "Epoch [7/10], Step [1800/1875], Loss: -3.7466, Training Accuracy: 95.80%\n",
      "Epoch [7/10], Training Accuracy: 95.86%\n",
      "Epoch [8/10], Step [200/1875], Loss: -3.7610, Training Accuracy: 96.38%\n",
      "Epoch [8/10], Step [400/1875], Loss: -3.7445, Training Accuracy: 96.09%\n",
      "Epoch [8/10], Step [600/1875], Loss: -3.7395, Training Accuracy: 96.08%\n",
      "Epoch [8/10], Step [800/1875], Loss: -3.7567, Training Accuracy: 96.10%\n",
      "Epoch [8/10], Step [1000/1875], Loss: -3.7531, Training Accuracy: 96.04%\n",
      "Epoch [8/10], Step [1200/1875], Loss: -3.7363, Training Accuracy: 96.13%\n",
      "Epoch [8/10], Step [1400/1875], Loss: -3.7614, Training Accuracy: 96.20%\n",
      "Epoch [8/10], Step [1600/1875], Loss: -3.7398, Training Accuracy: 96.15%\n",
      "Epoch [8/10], Step [1800/1875], Loss: -3.7511, Training Accuracy: 96.23%\n",
      "Epoch [8/10], Training Accuracy: 96.29%\n",
      "Epoch [9/10], Step [200/1875], Loss: -3.7631, Training Accuracy: 96.53%\n",
      "Epoch [9/10], Step [400/1875], Loss: -3.7498, Training Accuracy: 96.41%\n",
      "Epoch [9/10], Step [600/1875], Loss: -3.7442, Training Accuracy: 96.35%\n",
      "Epoch [9/10], Step [800/1875], Loss: -3.7595, Training Accuracy: 96.38%\n",
      "Epoch [9/10], Step [1000/1875], Loss: -3.7576, Training Accuracy: 96.31%\n",
      "Epoch [9/10], Step [1200/1875], Loss: -3.7399, Training Accuracy: 96.39%\n",
      "Epoch [9/10], Step [1400/1875], Loss: -3.7638, Training Accuracy: 96.43%\n",
      "Epoch [9/10], Step [1600/1875], Loss: -3.7435, Training Accuracy: 96.39%\n",
      "Epoch [9/10], Step [1800/1875], Loss: -3.7547, Training Accuracy: 96.45%\n",
      "Epoch [9/10], Training Accuracy: 96.51%\n",
      "Epoch [10/10], Step [200/1875], Loss: -3.7643, Training Accuracy: 96.84%\n",
      "Epoch [10/10], Step [400/1875], Loss: -3.7541, Training Accuracy: 96.68%\n",
      "Epoch [10/10], Step [600/1875], Loss: -3.7478, Training Accuracy: 96.67%\n",
      "Epoch [10/10], Step [800/1875], Loss: -3.7614, Training Accuracy: 96.68%\n",
      "Epoch [10/10], Step [1000/1875], Loss: -3.7610, Training Accuracy: 96.59%\n",
      "Epoch [10/10], Step [1200/1875], Loss: -3.7426, Training Accuracy: 96.67%\n",
      "Epoch [10/10], Step [1400/1875], Loss: -3.7660, Training Accuracy: 96.68%\n",
      "Epoch [10/10], Step [1600/1875], Loss: -3.7464, Training Accuracy: 96.63%\n",
      "Epoch [10/10], Step [1800/1875], Loss: -3.7579, Training Accuracy: 96.69%\n",
      "Epoch [10/10], Training Accuracy: 96.74%\n"
     ]
    }
   ],
   "source": [
    "# Define relevant variables\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet5(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "print(f'Total steps: {total_step}')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(\"Input image min/max:\", images.min(), images.max())\n",
    "        # import matplotlib.pyplot as plt\n",
    "\n",
    "        # Assuming 'image' is your input image\n",
    "        # plt.imshow(images[0].squeeze(0), cmap='gray')  # Display image in grayscale\n",
    "        # plt.colorbar()  # Add a color bar to show the range of pixel values\n",
    "        # plt.title(\"Image Preview\")  # Optional title\n",
    "        # plt.show()\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(labels.shape, outputs.shape)\n",
    "        # print(\"Labels (sample):\", labels[0])\n",
    "        # max_value, predicted_class = torch.max(outputs.data, 1)\n",
    "\n",
    "        # print(f\"Predicted class: {predicted_class[0]}, with value: {max_value[0].item()}\")\n",
    "\n",
    "        # print(\"Raw outputs (distances) sample:\", outputs[0])  # One example\n",
    "        # print(\"Output shape:\", outputs.shape)\n",
    "        # loss_fn = nn.CrossEntropyLoss()\n",
    "        # loss = loss_fn(outputs, labels)\n",
    "        loss = customLoss(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tracking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            accuracy = 100 * correct / total\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Training Accuracy: {:.2f}%'.format(epoch+1, num_epochs, i+1, total_step, loss.item(), accuracy))\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "22b58a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.85%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeNet5-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
