{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdTzfiZkkSgt"
   },
   "source": [
    "### LeNet5 1998 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siRujQdNkKIX"
   },
   "source": [
    "#### 1. Calculating the mean and variance of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eqUYcp-jQZC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "Recalculated Mean (train_loader): 0.066592\n",
      "Recalculated Std (train_loader): 0.392837\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from data import splits, df_train, df_test\n",
    "\n",
    "# Convert 'image' column to NumPy arrays by extracting pixel data\n",
    "df_train['image'] = df_train['image'].apply(lambda x: np.array(x['bytes']) if isinstance(x, dict) and 'bytes' in x else np.array(x))\n",
    "df_test['image'] = df_test['image'].apply(lambda x: np.array(x['bytes']) if isinstance(x, dict) and 'bytes' in x else np.array(x))\n",
    "\n",
    "# Create a custom dataset class to handle this data\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image data (assuming it's stored in columns 'image')\n",
    "        image = self.dataframe.iloc[idx]['image']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        \n",
    "        # Convert the image (which is assumed to be a 28x28 numpy array) to a PIL image\n",
    "        image = Image.fromarray(image.astype(np.uint8), mode='L')\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define the transformation (normalize according to the given formula)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor (scales to [0, 1])\n",
    "    transforms.Lambda(lambda x: x * 1.275 - 0.1)  # Normalize as per the given formula\n",
    "])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = CustomMNISTDataset(dataframe=df_train, transform=transform)\n",
    "test_dataset = CustomMNISTDataset(dataframe=df_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "\n",
    "all_pixels = []\n",
    "\n",
    "# Use the train_loader (not digit_loader) since this is the dataset you trained on\n",
    "for images, _ in train_loader:\n",
    "    pixels = images.view(images.size(0), -1)  # Flatten images\n",
    "    all_pixels.append(pixels)\n",
    "\n",
    "all_pixels = torch.cat(all_pixels, dim=0)  # (N, 28*28) or (N, 32*32) if resized\n",
    "\n",
    "# Compute mean and std\n",
    "mean = all_pixels.mean()\n",
    "std = all_pixels.std()\n",
    "\n",
    "print(f\"Recalculated Mean (train_loader): {mean.item():.6f}\")\n",
    "print(f\"Recalculated Std (train_loader): {std.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Saving the MNIST data to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create the data directory and subdirectories\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "\n",
    "# Save training images and labels\n",
    "with open(\"./data/train_label.txt\", \"w\") as train_label_file:\n",
    "    for idx, row in df_train.iterrows():\n",
    "        # Save the image\n",
    "        image = Image.fromarray(row['image'].astype('uint8'))\n",
    "        image.save(f\"./data/train/{idx}.png\")\n",
    "        \n",
    "        # Save the label\n",
    "        train_label_file.write(f\"{row['label']}\\n\")\n",
    "\n",
    "# Save testing images and labels\n",
    "with open(\"./data/test_label.txt\", \"w\") as test_label_file:\n",
    "    for idx, row in df_test.iterrows():\n",
    "        # Save the image\n",
    "        image = Image.fromarray(row['image'].astype('uint8')) \n",
    "        image.save(f\"./data/test/{idx}.png\")\n",
    "        \n",
    "        # Save the label\n",
    "        test_label_file.write(f\"{row['label']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvQvd-eRkbLk"
   },
   "source": [
    "#### 3. Building the LeNet5 1998 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "O8topbYkh4UV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "class LeNet5_S2Layer(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(LeNet5_S2Layer, self).__init__()\n",
    "        self.coefficient = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        pooled = pooled * self.coefficient.view(1, -1, 1, 1)\n",
    "        pooled = pooled + self.bias.view(1, -1, 1, 1)\n",
    "        return torch.sigmoid(pooled)\n",
    "\n",
    "class ScaledTanh(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1.7159 * torch.tanh(x * 2 / 3)\n",
    "\n",
    "class SquashingFunction(nn.Module):\n",
    "    def __init__(self, A=1.7159, S=2/3):\n",
    "        super(SquashingFunction, self).__init__()\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.S * x\n",
    "        x = torch.clamp(x, min=-0.999, max=0.999)  # avoid NaNs from atanh\n",
    "        return self.A * torch.atanh(x)\n",
    "\n",
    "class C3PartialConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, connection_table):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.connection_table = connection_table\n",
    "\n",
    "        # All connections are initialized, then masked\n",
    "        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        self.mask = torch.zeros_like(self.weight)\n",
    "\n",
    "        # Build the binary mask\n",
    "        for out_c, in_list in enumerate(connection_table):\n",
    "            for in_c in in_list:\n",
    "                self.mask[out_c, in_c, :, :] = 1.0\n",
    "\n",
    "        # Initialize only the allowed weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        nn.init.uniform_(self.weight, -2.4 / fan_in, 2.4 / fan_in)\n",
    "        self.bias.data.fill_(2.4 / fan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply mask before convolution to zero-out unwanted connections\n",
    "        masked_weight = self.weight * self.mask.to(self.weight.device)\n",
    "        return F.conv2d(x, masked_weight, self.bias, stride=1)\n",
    "\n",
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Loop through all class labels (0-9)\n",
    "        for label in range(10):\n",
    "            class_dir = os.path.join(image_dir, str(label))\n",
    "            for image_name in os.listdir(class_dir):\n",
    "                image_path = os.path.join(class_dir, image_name)\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale ('L' mode)\n",
    "\n",
    "        # Apply transformation (resize, tensor conversion, normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "class EuclideanRBFOutput(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_dim=84):\n",
    "        super(EuclideanRBFOutput, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, input_dim)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        centers = self.centers.unsqueeze(0)\n",
    "        distances = torch.sum((x - centers) ** 2, dim=2)\n",
    "        return -distances\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # C1 and S2\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            ScaledTanh(),\n",
    "            LeNet5_S2Layer(6)\n",
    "        )\n",
    "\n",
    "        # C3 and S4\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            ScaledTanh(),\n",
    "            LeNet5_S2Layer(16)\n",
    "        )\n",
    "\n",
    "        # C5\n",
    "        self.layer3 = C3PartialConv(in_channels=16, out_channels=120, kernel_size=5, connection_table=[random.sample(range(16), 5) for _ in range(120)])\n",
    "\n",
    "        # F6\n",
    "        self.fc = nn.Linear(120, 84)\n",
    "        self.squashing = SquashingFunction(A=1.0, S=1.0)\n",
    "\n",
    "        # RBF Output layer\n",
    "        self.rbf_output = EuclideanRBFOutput(num_classes=num_classes, input_dim=84)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.squashing(out)\n",
    "        out = self.rbf_output(out)\n",
    "        return out\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.squashing(x)\n",
    "        return x\n",
    "\n",
    "def extract_features_from_digit_dataset(model, digit_loader, device):\n",
    "    model.eval()\n",
    "    features_by_class = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in digit_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            feats = model.extract_features(images)\n",
    "            \n",
    "            for f, label in zip(feats, labels):\n",
    "                features_by_class[label.item()].append(f.cpu().numpy())\n",
    "    \n",
    "    return features_by_class\n",
    "\n",
    "# computing RBF parameters by finding the mean of the features for each class\n",
    "def compute_rbf_centers(features_by_class, num_classes, input_dim):\n",
    "    centers = np.zeros((num_classes, input_dim), dtype=np.float32)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        class_feats = np.stack(features_by_class[cls])\n",
    "        centers[cls] = np.mean(class_feats, axis=0)\n",
    "    \n",
    "    return torch.tensor(centers, dtype=torch.float32)\n",
    "    \n",
    "class SafeTensorResize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # If it's a PIL Image, convert it to tensor\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        # Resize\n",
    "        return F.interpolate(img.unsqueeze(0), size=self.size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "# Final unified transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale (1 channel)\n",
    "    SafeTensorResize((32, 32)),\n",
    "    transforms.Normalize(mean=(0.066592,), std=(0.392837,))\n",
    "])\n",
    "\n",
    "class CustomLeNetLoss(nn.Module):\n",
    "    def __init__(self, j=0.1):\n",
    "        super(CustomLeNetLoss, self).__init__()\n",
    "        self.j = j  # Scaling factor for incorrect classes\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Apply softmax to logits to get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        num_classes = logits.size(1)\n",
    "\n",
    "        # Get the log probabilities (log(p))\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        # Extract the probabilities for the correct class\n",
    "        correct_class_log_probs = log_probs.gather(1, labels.view(-1, 1))\n",
    "        \n",
    "        # Loss for the correct class (negative log probability)\n",
    "        correct_class_loss = -correct_class_log_probs\n",
    "        \n",
    "        # Loss for the incorrect classes (scaled by j)\n",
    "        incorrect_class_log_probs = log_probs.clone()\n",
    "        incorrect_class_log_probs.scatter_(1, labels.view(-1, 1), 0)\n",
    "        \n",
    "        # Apply the scaling factor j to the incorrect class log probabilities\n",
    "        incorrect_class_loss = self.j * torch.sum(torch.exp(incorrect_class_log_probs) * log_probs, dim=1, keepdim=True)\n",
    "        \n",
    "        # Combine the losses: correct class loss + incorrect class loss\n",
    "        loss = correct_class_loss + incorrect_class_loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLzPIMBfkg39"
   },
   "source": [
    "#### 4. Loading in the MNIST data to split into training and testing data sets, and loading in the Digits data to calculate the RBF parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "J0Z2e6nIjhsl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the model on MNIST data:\n",
    "\n",
    "from mnist import MNIST\n",
    "\n",
    "# Define relevant variables\n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_dataset = MNIST(split=\"train\", transform=transform)\n",
    "test_dataset = MNIST(split=\"test\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "image_dir = './digits updated/'\n",
    "\n",
    "digit_dataset = DigitDataset(image_dir=image_dir, transform=transform)\n",
    "digit_loader = DataLoader(digit_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "features_by_class = extract_features_from_digit_dataset(model, digit_loader, device)\n",
    "\n",
    "# Compute RBF centers using the features from DIGIT\n",
    "rbf_centers = compute_rbf_centers(features_by_class, num_classes=10, input_dim=84)\n",
    "\n",
    "# Set the computed centers into the model\n",
    "model.rbf_output.centers.data = rbf_centers.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl0LmTonknwb"
   },
   "source": [
    "#### 5. Training the model on MNIST data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "6jOwihTljq_r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [157/157], Loss: 0.7502\n",
      "Epoch [1/30], Training Accuracy: 29.25%\n",
      "Epoch [2/30], Step [157/157], Loss: 0.6044\n",
      "Epoch [2/30], Training Accuracy: 75.56%\n",
      "Epoch [3/30], Step [157/157], Loss: 0.0930\n",
      "Epoch [3/30], Training Accuracy: 84.66%\n",
      "Epoch [4/30], Step [157/157], Loss: 0.7556\n",
      "Epoch [4/30], Training Accuracy: 88.05%\n",
      "Epoch [5/30], Step [157/157], Loss: 0.0198\n",
      "Epoch [5/30], Training Accuracy: 90.83%\n",
      "Epoch [6/30], Step [157/157], Loss: 0.4451\n",
      "Epoch [6/30], Training Accuracy: 91.40%\n",
      "Epoch [7/30], Step [157/157], Loss: 0.1116\n",
      "Epoch [7/30], Training Accuracy: 92.93%\n",
      "Epoch [8/30], Step [157/157], Loss: 0.0274\n",
      "Epoch [8/30], Training Accuracy: 93.42%\n",
      "Epoch [9/30], Step [157/157], Loss: 0.2799\n",
      "Epoch [9/30], Training Accuracy: 94.18%\n",
      "Epoch [10/30], Step [157/157], Loss: 0.0498\n",
      "Epoch [10/30], Training Accuracy: 94.69%\n",
      "Epoch [11/30], Step [157/157], Loss: 0.0962\n",
      "Epoch [11/30], Training Accuracy: 94.78%\n",
      "Epoch [12/30], Step [157/157], Loss: 0.1251\n",
      "Epoch [12/30], Training Accuracy: 95.03%\n",
      "Epoch [13/30], Step [157/157], Loss: 0.1840\n",
      "Epoch [13/30], Training Accuracy: 95.46%\n",
      "Epoch [14/30], Step [157/157], Loss: 0.2672\n",
      "Epoch [14/30], Training Accuracy: 95.92%\n",
      "Epoch [15/30], Step [157/157], Loss: 0.0077\n",
      "Epoch [15/30], Training Accuracy: 96.14%\n",
      "Epoch [16/30], Step [157/157], Loss: 0.0425\n",
      "Epoch [16/30], Training Accuracy: 96.16%\n",
      "Epoch [17/30], Step [157/157], Loss: 0.3764\n",
      "Epoch [17/30], Training Accuracy: 96.53%\n",
      "Epoch [18/30], Step [157/157], Loss: 0.0632\n",
      "Epoch [18/30], Training Accuracy: 96.52%\n",
      "Epoch [19/30], Step [157/157], Loss: 0.0374\n",
      "Epoch [19/30], Training Accuracy: 95.99%\n",
      "Epoch [20/30], Step [157/157], Loss: 0.0194\n",
      "Epoch [20/30], Training Accuracy: 96.32%\n",
      "Epoch [21/30], Step [157/157], Loss: 0.1134\n",
      "Epoch [21/30], Training Accuracy: 96.96%\n",
      "Epoch [22/30], Step [157/157], Loss: 0.0456\n",
      "Epoch [22/30], Training Accuracy: 97.11%\n",
      "Epoch [23/30], Step [157/157], Loss: 0.0106\n",
      "Epoch [23/30], Training Accuracy: 97.36%\n",
      "Epoch [24/30], Step [157/157], Loss: 0.0277\n",
      "Epoch [24/30], Training Accuracy: 97.37%\n",
      "Epoch [25/30], Step [157/157], Loss: 0.0956\n",
      "Epoch [25/30], Training Accuracy: 97.48%\n",
      "Epoch [26/30], Step [157/157], Loss: 0.0247\n",
      "Epoch [26/30], Training Accuracy: 97.17%\n",
      "Epoch [27/30], Step [157/157], Loss: 0.1928\n",
      "Epoch [27/30], Training Accuracy: 97.47%\n",
      "Epoch [28/30], Step [157/157], Loss: 0.0193\n",
      "Epoch [28/30], Training Accuracy: 96.95%\n",
      "Epoch [29/30], Step [157/157], Loss: 0.0917\n",
      "Epoch [29/30], Training Accuracy: 97.32%\n",
      "Epoch [30/30], Step [157/157], Loss: 0.1152\n",
      "Epoch [30/30], Training Accuracy: 97.57%\n"
     ]
    }
   ],
   "source": [
    "# Training the model on MNIST data:\n",
    "\n",
    "features_by_class = extract_features_from_digit_dataset(model, digit_loader, device)\n",
    "\n",
    "# Initialize RBF centers from class means\n",
    "rbf_centers = compute_rbf_centers(features_by_class, num_classes=10, input_dim=84)\n",
    "model.rbf_output.centers.data = rbf_centers.to(device)\n",
    "\n",
    "cost = CustomLeNetLoss(j=0.1)\n",
    "\n",
    "# Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tracking accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) % len(train_loader) == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIyH489Ckr3y"
   },
   "source": [
    "#### 6. Testing the model on MNIST data subset produces ~97% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "x5GG_V1ah6Rq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Testing Accuracy: 96.88%\n",
      "Epoch [2/30], Testing Accuracy: 96.09%\n",
      "Epoch [3/30], Testing Accuracy: 97.40%\n",
      "Epoch [4/30], Testing Accuracy: 97.27%\n",
      "Epoch [5/30], Testing Accuracy: 97.50%\n",
      "Epoch [6/30], Testing Accuracy: 96.88%\n",
      "Epoch [7/30], Testing Accuracy: 96.88%\n",
      "Epoch [8/30], Testing Accuracy: 97.07%\n",
      "Epoch [9/30], Testing Accuracy: 96.53%\n",
      "Epoch [10/30], Testing Accuracy: 96.25%\n",
      "Epoch [11/30], Testing Accuracy: 96.02%\n",
      "Epoch [12/30], Testing Accuracy: 95.70%\n",
      "Epoch [13/30], Testing Accuracy: 95.91%\n",
      "Epoch [14/30], Testing Accuracy: 95.87%\n",
      "Epoch [15/30], Testing Accuracy: 95.62%\n",
      "Epoch [16/30], Testing Accuracy: 95.51%\n",
      "Epoch [17/30], Testing Accuracy: 95.59%\n",
      "Epoch [18/30], Testing Accuracy: 95.40%\n",
      "Epoch [19/30], Testing Accuracy: 95.31%\n",
      "Epoch [20/30], Testing Accuracy: 95.00%\n",
      "Epoch [21/30], Testing Accuracy: 95.01%\n",
      "Epoch [22/30], Testing Accuracy: 95.10%\n",
      "Epoch [23/30], Testing Accuracy: 95.11%\n",
      "Epoch [24/30], Testing Accuracy: 95.05%\n",
      "Epoch [25/30], Testing Accuracy: 95.00%\n",
      "Epoch [26/30], Testing Accuracy: 95.07%\n",
      "Epoch [27/30], Testing Accuracy: 94.97%\n",
      "Epoch [28/30], Testing Accuracy: 94.98%\n",
      "Epoch [29/30], Testing Accuracy: 94.99%\n",
      "Epoch [30/30], Testing Accuracy: 95.10%\n",
      "Epoch [31/30], Testing Accuracy: 95.26%\n",
      "Epoch [32/30], Testing Accuracy: 95.07%\n",
      "Epoch [33/30], Testing Accuracy: 95.08%\n",
      "Epoch [34/30], Testing Accuracy: 95.04%\n",
      "Epoch [35/30], Testing Accuracy: 95.04%\n",
      "Epoch [36/30], Testing Accuracy: 95.05%\n",
      "Epoch [37/30], Testing Accuracy: 95.14%\n",
      "Epoch [38/30], Testing Accuracy: 94.94%\n",
      "Epoch [39/30], Testing Accuracy: 94.91%\n",
      "Epoch [40/30], Testing Accuracy: 95.04%\n",
      "Epoch [41/30], Testing Accuracy: 95.05%\n",
      "Epoch [42/30], Testing Accuracy: 95.05%\n",
      "Epoch [43/30], Testing Accuracy: 95.13%\n",
      "Epoch [44/30], Testing Accuracy: 95.17%\n",
      "Epoch [45/30], Testing Accuracy: 95.24%\n",
      "Epoch [46/30], Testing Accuracy: 95.18%\n",
      "Epoch [47/30], Testing Accuracy: 95.08%\n",
      "Epoch [48/30], Testing Accuracy: 95.08%\n",
      "Epoch [49/30], Testing Accuracy: 94.96%\n",
      "Epoch [50/30], Testing Accuracy: 95.06%\n",
      "Epoch [51/30], Testing Accuracy: 95.07%\n",
      "Epoch [52/30], Testing Accuracy: 95.10%\n",
      "Epoch [53/30], Testing Accuracy: 95.14%\n",
      "Epoch [54/30], Testing Accuracy: 95.20%\n",
      "Epoch [55/30], Testing Accuracy: 95.23%\n",
      "Epoch [56/30], Testing Accuracy: 95.20%\n",
      "Epoch [57/30], Testing Accuracy: 95.23%\n",
      "Epoch [58/30], Testing Accuracy: 95.26%\n",
      "Epoch [59/30], Testing Accuracy: 95.26%\n",
      "Epoch [60/30], Testing Accuracy: 95.21%\n",
      "Epoch [61/30], Testing Accuracy: 95.13%\n",
      "Epoch [62/30], Testing Accuracy: 95.11%\n",
      "Epoch [63/30], Testing Accuracy: 95.11%\n",
      "Epoch [64/30], Testing Accuracy: 95.07%\n",
      "Epoch [65/30], Testing Accuracy: 95.07%\n",
      "Epoch [66/30], Testing Accuracy: 94.98%\n",
      "Epoch [67/30], Testing Accuracy: 94.96%\n",
      "Epoch [68/30], Testing Accuracy: 94.97%\n",
      "Epoch [69/30], Testing Accuracy: 94.97%\n",
      "Epoch [70/30], Testing Accuracy: 95.02%\n",
      "Epoch [71/30], Testing Accuracy: 95.00%\n",
      "Epoch [72/30], Testing Accuracy: 94.97%\n",
      "Epoch [73/30], Testing Accuracy: 95.01%\n",
      "Epoch [74/30], Testing Accuracy: 95.02%\n",
      "Epoch [75/30], Testing Accuracy: 95.00%\n",
      "Epoch [76/30], Testing Accuracy: 94.96%\n",
      "Epoch [77/30], Testing Accuracy: 94.99%\n",
      "Epoch [78/30], Testing Accuracy: 94.99%\n",
      "Epoch [79/30], Testing Accuracy: 95.06%\n",
      "Epoch [80/30], Testing Accuracy: 95.08%\n",
      "Epoch [81/30], Testing Accuracy: 95.08%\n",
      "Epoch [82/30], Testing Accuracy: 95.14%\n",
      "Epoch [83/30], Testing Accuracy: 95.18%\n",
      "Epoch [84/30], Testing Accuracy: 95.22%\n",
      "Epoch [85/30], Testing Accuracy: 95.28%\n",
      "Epoch [86/30], Testing Accuracy: 95.31%\n",
      "Epoch [87/30], Testing Accuracy: 95.35%\n",
      "Epoch [88/30], Testing Accuracy: 95.35%\n",
      "Epoch [89/30], Testing Accuracy: 95.38%\n",
      "Epoch [90/30], Testing Accuracy: 95.36%\n",
      "Epoch [91/30], Testing Accuracy: 95.42%\n",
      "Epoch [92/30], Testing Accuracy: 95.45%\n",
      "Epoch [93/30], Testing Accuracy: 95.43%\n",
      "Epoch [94/30], Testing Accuracy: 95.40%\n",
      "Epoch [95/30], Testing Accuracy: 95.36%\n",
      "Epoch [96/30], Testing Accuracy: 95.36%\n",
      "Epoch [97/30], Testing Accuracy: 95.34%\n",
      "Epoch [98/30], Testing Accuracy: 95.39%\n",
      "Epoch [99/30], Testing Accuracy: 95.44%\n",
      "Epoch [100/30], Testing Accuracy: 95.48%\n",
      "Epoch [101/30], Testing Accuracy: 95.51%\n",
      "Epoch [102/30], Testing Accuracy: 95.56%\n",
      "Epoch [103/30], Testing Accuracy: 95.57%\n",
      "Epoch [104/30], Testing Accuracy: 95.55%\n",
      "Epoch [105/30], Testing Accuracy: 95.60%\n",
      "Epoch [106/30], Testing Accuracy: 95.62%\n",
      "Epoch [107/30], Testing Accuracy: 95.66%\n",
      "Epoch [108/30], Testing Accuracy: 95.69%\n",
      "Epoch [109/30], Testing Accuracy: 95.73%\n",
      "Epoch [110/30], Testing Accuracy: 95.77%\n",
      "Epoch [111/30], Testing Accuracy: 95.81%\n",
      "Epoch [112/30], Testing Accuracy: 95.83%\n",
      "Epoch [113/30], Testing Accuracy: 95.87%\n",
      "Epoch [114/30], Testing Accuracy: 95.90%\n",
      "Epoch [115/30], Testing Accuracy: 95.94%\n",
      "Epoch [116/30], Testing Accuracy: 95.97%\n",
      "Epoch [117/30], Testing Accuracy: 95.97%\n",
      "Epoch [118/30], Testing Accuracy: 95.95%\n",
      "Epoch [119/30], Testing Accuracy: 95.97%\n",
      "Epoch [120/30], Testing Accuracy: 95.99%\n",
      "Epoch [121/30], Testing Accuracy: 96.02%\n",
      "Epoch [122/30], Testing Accuracy: 96.06%\n",
      "Epoch [123/30], Testing Accuracy: 96.07%\n",
      "Epoch [124/30], Testing Accuracy: 96.09%\n",
      "Epoch [125/30], Testing Accuracy: 96.12%\n",
      "Epoch [126/30], Testing Accuracy: 96.16%\n",
      "Epoch [127/30], Testing Accuracy: 96.16%\n",
      "Epoch [128/30], Testing Accuracy: 96.17%\n",
      "Epoch [129/30], Testing Accuracy: 96.18%\n",
      "Epoch [130/30], Testing Accuracy: 96.20%\n",
      "Epoch [131/30], Testing Accuracy: 96.21%\n",
      "Epoch [132/30], Testing Accuracy: 96.24%\n",
      "Epoch [133/30], Testing Accuracy: 96.26%\n",
      "Epoch [134/30], Testing Accuracy: 96.27%\n",
      "Epoch [135/30], Testing Accuracy: 96.30%\n",
      "Epoch [136/30], Testing Accuracy: 96.32%\n",
      "Epoch [137/30], Testing Accuracy: 96.35%\n",
      "Epoch [138/30], Testing Accuracy: 96.38%\n",
      "Epoch [139/30], Testing Accuracy: 96.40%\n",
      "Epoch [140/30], Testing Accuracy: 96.43%\n",
      "Epoch [141/30], Testing Accuracy: 96.41%\n",
      "Epoch [142/30], Testing Accuracy: 96.41%\n",
      "Epoch [143/30], Testing Accuracy: 96.44%\n",
      "Epoch [144/30], Testing Accuracy: 96.46%\n",
      "Epoch [145/30], Testing Accuracy: 96.49%\n",
      "Epoch [146/30], Testing Accuracy: 96.50%\n",
      "Epoch [147/30], Testing Accuracy: 96.52%\n",
      "Epoch [148/30], Testing Accuracy: 96.55%\n",
      "Epoch [149/30], Testing Accuracy: 96.57%\n",
      "Epoch [150/30], Testing Accuracy: 96.59%\n",
      "Epoch [151/30], Testing Accuracy: 96.60%\n",
      "Epoch [152/30], Testing Accuracy: 96.60%\n",
      "Epoch [153/30], Testing Accuracy: 96.55%\n",
      "Epoch [154/30], Testing Accuracy: 96.53%\n",
      "Epoch [155/30], Testing Accuracy: 96.52%\n",
      "Epoch [156/30], Testing Accuracy: 96.51%\n",
      "Epoch [157/30], Testing Accuracy: 96.52%\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on MNIST data:\n",
    "import importlib\n",
    "import mnist\n",
    "importlib.reload(mnist)\n",
    "from mnist import MNIST\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    idx = 0\n",
    "    for images, labels in test_loader:\n",
    "        idx += 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{idx}/{num_epochs}], Testing Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LeNet5-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
